{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment_1_SET4_IR Group 036.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muzOgNnJm99v"
      },
      "source": [
        "# Loading the data set and import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6AI1lg8xL4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e8cdbb-a40f-401d-a884-f74f510ca327"
      },
      "source": [
        "# importing necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import *\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.cluster import KMeans\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgiqtNhVmjwO"
      },
      "source": [
        "# downloading the data\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download paultimothymooney/cvpr-2019-papers/CVPR2019/abstracts\n",
        "! unzip cvpr-2019-papers.zip\n",
        "! rm -r CVPR2019/papers\n",
        "! rm -r cvpr-2019-papers.zip\n",
        "! rm -r cvpr2019\n",
        "! ls CVPR2019"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPxpjJkBmq4E"
      },
      "source": [
        "# loading the data and splitting into train and test\n",
        "abs_dir = 'CVPR2019/abstracts/'\n",
        "\n",
        "data = os.listdir(abs_dir)\n",
        "train_files, test_files = train_test_split(data, test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuULOPeEnj9N"
      },
      "source": [
        "# Performing the vectorization process  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4CmYlLqmDyu"
      },
      "source": [
        "# reading text from files\n",
        "stop = stopwords.words('english')\n",
        "train_txt, test_txt = [], []\n",
        "\n",
        "for doc_name in train_files:\n",
        "    with open(abs_dir + doc_name) as file:\n",
        "        doc = \"\"\n",
        "        for line in file.readlines():\n",
        "          for word in line.split()[:-1]:\n",
        "            if word not in stop:\n",
        "              doc = doc + ' ' + word.lower()\n",
        "        train_txt.append(doc)\n",
        "\n",
        "\n",
        "for test_name in test_files:\n",
        "    with open(abs_dir + test_name) as file:\n",
        "        query = \"\"\n",
        "        for line in file.readlines():\n",
        "          for word in line.split()[:-1]:\n",
        "            if word not in stop:\n",
        "              query = query + ' '+ word.lower()\n",
        "        test_txt.append(query.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLWV_FLHn-EX"
      },
      "source": [
        "# Computing TF and TF-IDF factors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUrRNHzIYFtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb423c96-54a1-4f86-9cf7-19e22cf49456"
      },
      "source": [
        "# vectrorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# calculating tf-idf values\n",
        "doc_tfidfs = vectorizer.fit_transform(train_txt).toarray()\n",
        "query_vecs = vectorizer.transform(test_txt).toarray()\n",
        "\n",
        "# Rank documents based on cosine similarity\n",
        "cos_sim = cosine_similarity(query_vecs, doc_tfidfs)\n",
        "rankings = np.flip(cos_sim.argsort(), axis=1)\n",
        "\n",
        "print(cos_sim)\n",
        "print(rankings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.05133055 0.06431965 0.04127186 ... 0.02359016 0.02970632 0.02256956]\n",
            " [0.08876582 0.08910752 0.01787912 ... 0.04171894 0.06443052 0.01945204]\n",
            " [0.06526623 0.03643963 0.02032667 ... 0.02998894 0.02173858 0.01973819]\n",
            " ...\n",
            " [0.08294365 0.01124988 0.02754354 ... 0.01861755 0.08112373 0.02089505]\n",
            " [0.02190253 0.00780802 0.03914401 ... 0.03890883 0.04551046 0.02088216]\n",
            " [0.0115889  0.00982963 0.00496501 ... 0.01828689 0.01292194 0.052333  ]]\n",
            "[[670 459 188 ... 539  52 833]\n",
            " [636 640 388 ... 614  75 833]\n",
            " [272 460 421 ... 694 258 833]\n",
            " ...\n",
            " [132 593 448 ... 890 753 833]\n",
            " [801 489 238 ...  42  32 833]\n",
            " [107  78  47 ...  19 548 833]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrBhYpijoPEk"
      },
      "source": [
        "# text classification by using Rocchio algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUVymVS0Qism",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37a2113-9197-4dee-a84a-277cb6f9eb77"
      },
      "source": [
        "# Rocchio \n",
        "#(Below is a param set)\n",
        "alpha = 1\n",
        "beta = 0.75\n",
        "gamma = 0.15\n",
        "rel_count = 5   # Use top-5 relevant documents to update query vector.\n",
        "nrel_count = 1  # Use only the most non-relevant document to update query vector.\n",
        "iters = 5\n",
        "for _ in range(iters):\n",
        "    \n",
        "    # Update query vectors with Rocchio algorithm\n",
        "    rel_vecs = doc_tfidfs[rankings[:, :rel_count]].mean(axis=1)\n",
        "    nrel_vecs = doc_tfidfs[rankings[:, -nrel_count:]].mean(axis=1)\n",
        "    query_vecs = alpha * query_vecs + beta * rel_vecs - gamma * nrel_vecs\n",
        "    \n",
        "    # Rerank documents based on cosine similarity\n",
        "    cos_sim = cosine_similarity(query_vecs, doc_tfidfs)\n",
        "    rankings = np.flip(cos_sim.argsort(axis=1), axis=1)\n",
        "\n",
        "print(cos_sim)\n",
        "print(rankings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.10115727 0.08675012 0.06462676 ... 0.0507675  0.05787577 0.04101204]\n",
            " [0.07681489 0.09306108 0.04355687 ... 0.045645   0.06219576 0.0352161 ]\n",
            " [0.07943071 0.03727308 0.03310668 ... 0.04253915 0.03153237 0.0200157 ]\n",
            " ...\n",
            " [0.15226483 0.03140234 0.04937466 ... 0.03947287 0.10457395 0.03472491]\n",
            " [0.08983933 0.04418926 0.07559159 ... 0.05230113 0.08816972 0.04721661]\n",
            " [0.04235981 0.03230935 0.01980204 ... 0.0334429  0.05017432 0.0354665 ]]\n",
            "[[188 670 359 ... 352 828 833]\n",
            " [640 636 946 ... 588 234 833]\n",
            " [272 238 460 ... 443 917 833]\n",
            " ...\n",
            " [132 593 229 ... 783 149 833]\n",
            " [801 713 994 ...  32 757 833]\n",
            " [107  78 372 ... 605 234 833]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gql5iB-epYYz",
        "outputId": "59a36332-370c-4924-be3a-ddc7a54fa929"
      },
      "source": [
        "# retrieving the list of related documents from train for given test query\n",
        "query_result_df = pd.DataFrame(columns=['Query','RetrievedDocuments'])\n",
        "for query_name, ranking in zip(test_files, rankings):\n",
        "  ranked_docs = ' '.join([train_files[idx] for idx in ranking])\n",
        "  query_result_df.loc[len(query_result_df.index)]= [query_name, ranked_docs]\n",
        "\n",
        "print(query_result_df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               Query                                 RetrievedDocuments\n",
            "0  Bapat_The_Domain_Transform_Solver_CVPR_2019_pa...  He_ODE-Inspired_Network_Design_for_Single_Imag...\n",
            "1  Porzi_Seamless_Scene_Segmentation_CVPR_2019_pa...  Kirillov_Panoptic_Feature_Pyramid_Networks_CVP...\n",
            "2  Li_Fully_Quantized_Network_for_Object_Detectio...  Yang_Quantization_Networks_CVPR_2019_paper.txt...\n",
            "3  Tonioni_Real-Time_Self-Adaptive_Deep_Stereo_CV...  Tonioni_Learning_to_Adapt_for_Stereo_CVPR_2019...\n",
            "4  Hou_3D-SIS_3D_Semantic_Instance_Segmentation_o...  Chen_Unsupervised_3D_Pose_Estimation_With_Geom...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZO7BOvcolRd"
      },
      "source": [
        "# Performing the stemming process on the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJTYgTo7V8F5"
      },
      "source": [
        "# Porter stemmer function\n",
        "porterStemmer = PorterStemmer()\n",
        "def getStemmerSentnce(sentence):\n",
        "  wordList = nltk.word_tokenize(sentence)\n",
        "  stemWords = [porterStemmer.stem(word) for word in wordList]\n",
        "  return ' '.join(stemWords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gue-sqaLW3Gr"
      },
      "source": [
        "# performing stemming on the data set\n",
        "stemm_train_txt, stemm_test_txt = [], []\n",
        "\n",
        "for doc_name in train_files:\n",
        "    with open(abs_dir + doc_name) as file:\n",
        "        doc = ' '.join([word for line in file.readlines() for word in line.split()[:-1]])\n",
        "        stemmed_doc = getStemmerSentnce(doc)\n",
        "        # print (stemmed_doc)\n",
        "        stemm_train_txt.append(stemmed_doc)\n",
        "\n",
        "for test_name in test_files:\n",
        "    with open(abs_dir + test_name) as file:\n",
        "        query = ' '.join([word for line in file.readlines() for word in line.split()[:-1]])\n",
        "        stemmed_query = getStemmerSentnce(query)\n",
        "        stemm_test_txt.append(stemmed_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUiARhP2XaB5",
        "outputId": "31646e59-1fe0-4bdf-d294-368adb96d81a"
      },
      "source": [
        "# calculationg TF-IDF values for stemmed text\n",
        "stemm_doc_tfidfs = vectorizer.fit_transform(stemm_train_txt).toarray()\n",
        "stemm_query_vecs = vectorizer.transform(stemm_test_txt).toarray()\n",
        "\n",
        "# Rank documents based on cosine similarity\n",
        "stemm_cos_sim = cosine_similarity(stemm_query_vecs, stemm_doc_tfidfs)\n",
        "stemm_rankings = np.flip(cos_sim.argsort(), axis=1)\n",
        "\n",
        "\n",
        "print(stemm_cos_sim)\n",
        "print(stemm_rankings)\n",
        "print(stemm_cos_sim.shape)\n",
        "print(stemm_rankings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.16052659 0.13453684 0.15129964 ... 0.10705927 0.08970227 0.1104427 ]\n",
            " [0.16481207 0.14620062 0.07664601 ... 0.09347423 0.12447309 0.08357977]\n",
            " [0.13145318 0.07648278 0.0733354  ... 0.10526678 0.07793651 0.07963164]\n",
            " ...\n",
            " [0.17375719 0.08579837 0.09548839 ... 0.08326577 0.12795391 0.08188284]\n",
            " [0.1690227  0.07629702 0.18487406 ... 0.15675256 0.13781531 0.12624728]\n",
            " [0.04340912 0.03636528 0.0423178  ... 0.05128383 0.04140397 0.07638388]]\n",
            "[[188 670 359 ... 352 828 833]\n",
            " [640 636 946 ... 588 234 833]\n",
            " [272 238 460 ... 443 917 833]\n",
            " ...\n",
            " [132 593 229 ... 783 149 833]\n",
            " [801 713 994 ...  32 757 833]\n",
            " [107  78 372 ... 605 234 833]]\n",
            "(259, 1034)\n",
            "(259, 1034)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pvpfc3fY0hu",
        "outputId": "9244c279-9b19-4d10-be16-522d6558a942"
      },
      "source": [
        "## Rocchio algorithm on stemmed data\n",
        "for _ in range(iters):    \n",
        "    # Update query vectors with Rocchio algorithm\n",
        "    rel_vecs = stemm_doc_tfidfs[rankings[:, :rel_count]].mean(axis=1)\n",
        "    nrel_vecs = stemm_doc_tfidfs[rankings[:, -nrel_count:]].mean(axis=1)\n",
        "    stemm_query_vecs = alpha * stemm_query_vecs + beta * rel_vecs - gamma * nrel_vecs\n",
        "    \n",
        "    # Rerank documents based on cosine similarity\n",
        "    stemm_cos_sim = cosine_similarity(stemm_query_vecs, stemm_doc_tfidfs)\n",
        "    stemm_rankings = np.flip(cos_sim.argsort(axis=1), axis=1)\n",
        "\n",
        "print(stemm_cos_sim)\n",
        "print(stemm_rankings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.23966885 0.18155382 0.21478273 ... 0.1729929  0.15181651 0.15968309]\n",
            " [0.18486575 0.17742176 0.14930197 ... 0.13598976 0.15594939 0.12937837]\n",
            " [0.1762827  0.10821883 0.12593733 ... 0.14113646 0.10310372 0.10609686]\n",
            " ...\n",
            " [0.26361559 0.14243541 0.17866429 ... 0.15226703 0.20476739 0.14971517]\n",
            " [0.22796745 0.13987281 0.21959964 ... 0.1841974  0.17819565 0.17289597]\n",
            " [0.14169326 0.09795802 0.11704202 ... 0.11502973 0.13039205 0.11320644]]\n",
            "[[188 670 359 ... 352 828 833]\n",
            " [640 636 946 ... 588 234 833]\n",
            " [272 238 460 ... 443 917 833]\n",
            " ...\n",
            " [132 593 229 ... 783 149 833]\n",
            " [801 713 994 ...  32 757 833]\n",
            " [107  78 372 ... 605 234 833]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxnwTqwvtf_q"
      },
      "source": [
        "# Comparing the performance of Rocchio algorithm with KMeans algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zo_RElsoI3s",
        "outputId": "3ece3fb3-701e-4628-9c8c-94bc54dd531d"
      },
      "source": [
        "# vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "doc_tfidfs = vectorizer.fit_transform(train_txt).toarray()\n",
        "query_vecs = vectorizer.transform(test_txt).toarray()\n",
        "\n",
        "# KMeans algorithm\n",
        "true_k = 10\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
        "model.fit(doc_tfidfs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
              "       n_clusters=10, n_init=1, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCRSfM-KpEMC"
      },
      "source": [
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ_hHeZMpPOF",
        "outputId": "740414f4-fb16-44ff-a6e6-333b5d0b98ec"
      },
      "source": [
        "# printing clusters and its features\n",
        "for i in range(true_k):\n",
        " print(\"\\nCluster %d:\" % i),\n",
        " for ind in order_centroids[i, :10]:\n",
        "  print('%s'% terms[ind])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cluster 0:\n",
            "object\n",
            "detection\n",
            "segmentation\n",
            "network\n",
            "the\n",
            "we\n",
            "image\n",
            "learning\n",
            "networks\n",
            "accuracy\n",
            "\n",
            "Cluster 1:\n",
            "context\n",
            "features\n",
            "graph\n",
            "feature\n",
            "network\n",
            "convolution\n",
            "channel\n",
            "information\n",
            "semantic\n",
            "module\n",
            "\n",
            "Cluster 2:\n",
            "domain\n",
            "learning\n",
            "classes\n",
            "data\n",
            "adaptation\n",
            "image\n",
            "target\n",
            "semantic\n",
            "training\n",
            "shot\n",
            "\n",
            "Cluster 3:\n",
            "adversarial\n",
            "attacks\n",
            "attack\n",
            "defense\n",
            "image\n",
            "perturbations\n",
            "examples\n",
            "gradient\n",
            "method\n",
            "norm\n",
            "\n",
            "Cluster 4:\n",
            "video\n",
            "temporal\n",
            "action\n",
            "frame\n",
            "videos\n",
            "motion\n",
            "frames\n",
            "model\n",
            "flow\n",
            "the\n",
            "\n",
            "Cluster 5:\n",
            "resolution\n",
            "imaging\n",
            "light\n",
            "blur\n",
            "image\n",
            "high\n",
            "images\n",
            "low\n",
            "nlos\n",
            "super\n",
            "\n",
            "Cluster 6:\n",
            "3d\n",
            "pose\n",
            "2d\n",
            "shape\n",
            "we\n",
            "face\n",
            "object\n",
            "estimation\n",
            "model\n",
            "the\n",
            "\n",
            "Cluster 7:\n",
            "visual\n",
            "image\n",
            "attention\n",
            "question\n",
            "task\n",
            "vqa\n",
            "dialog\n",
            "we\n",
            "language\n",
            "models\n",
            "\n",
            "Cluster 8:\n",
            "depth\n",
            "stereo\n",
            "camera\n",
            "view\n",
            "method\n",
            "image\n",
            "estimation\n",
            "scene\n",
            "data\n",
            "we\n",
            "\n",
            "Cluster 9:\n",
            "point\n",
            "local\n",
            "cloud\n",
            "3d\n",
            "registration\n",
            "clouds\n",
            "density\n",
            "the\n",
            "matching\n",
            "points\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO_SpQH2p1Yj",
        "outputId": "c7c987d7-2630-4c9b-ff57-7e328843591f"
      },
      "source": [
        "print(\"Prediction\")\n",
        "predicted = model.predict(query_vecs)\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction\n",
            "[8 0 0 8 6 4 4 6 2 2 2 8 7 1 7 0 5 5 0 9 9 2 0 0 0 8 2 7 0 0 1 6 0 0 0 4 4\n",
            " 3 6 7 0 2 1 3 0 2 7 6 2 4 4 1 4 0 4 0 2 8 1 2 2 5 6 1 0 8 0 0 0 6 9 4 0 4\n",
            " 2 3 2 6 2 2 2 9 6 9 7 9 0 4 1 0 4 0 7 8 4 0 7 0 7 0 9 0 0 2 4 0 6 0 2 0 2\n",
            " 6 7 1 1 4 9 0 0 1 8 9 8 0 2 0 2 0 0 0 0 5 0 0 4 8 5 2 2 6 0 2 0 4 0 0 9 0\n",
            " 1 7 5 7 7 2 7 0 9 4 1 6 7 2 0 7 0 3 6 0 0 2 0 7 0 0 7 7 1 0 4 7 6 7 4 9 0\n",
            " 4 0 2 2 6 6 0 1 0 2 0 4 2 0 0 8 2 4 6 0 4 0 8 6 2 0 4 4 3 0 2 0 1 3 0 1 0\n",
            " 0 0 4 2 5 8 4 1 0 5 8 8 0 0 7 7 2 2 4 0 4 4 2 0 0 0 0 0 8 6 0 0 2 7 7 0 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV2qNFcWn8Ot"
      },
      "source": [
        ""
      ]
    }
  ]
}